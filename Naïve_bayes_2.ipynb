{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9B_EK4Me9ix"
      },
      "outputs": [],
      "source": [
        "# Q1. The problem asks for the probability that an employee is a smoker given that they use the health insurance plan.\n",
        "# This can be solved using conditional probability and Bayes' theorem.\n",
        "\n",
        "# P(Smoker | Uses Health Insurance) = P(Uses Health Insurance | Smoker) * P(Smoker) / P(Uses Health Insurance)\n",
        "\n",
        "# Given:\n",
        "P_Uses_Health_Insurance = 0.70  # 70% of employees use the health insurance plan\n",
        "P_Smoker_given_Uses_Health_Insurance = 0.40  # 40% of employees who use the plan are smokers\n",
        "\n",
        "# To find the probability P(Smoker | Uses Health Insurance), we directly use P(Smoker | Uses Health Insurance)\n",
        "P_Smoker_given_Uses_Health_Insurance = P_Smoker_given_Uses_Health_Insurance\n",
        "\n",
        "# The formula is already given, hence the probability is 0.40 or 40%.\n",
        "print(f\"The probability that an employee is a smoker given they use the health insurance plan: {P_Smoker_given_Uses_Health_Insurance}\")\n",
        "\n",
        "# Q2. The difference between Bernoulli Naive Bayes and Multinomial Naive Bayes:\n",
        "# - Bernoulli Naive Bayes is used when the features are binary (0 or 1) and models binary events.\n",
        "# - Multinomial Naive Bayes is used for discrete counts (e.g., word counts in text classification).\n",
        "#   It assumes the features represent the frequencies of events.\n",
        "\n",
        "# Q3. How does Bernoulli Naive Bayes handle missing values?\n",
        "# Bernoulli Naive Bayes requires binary features, so missing values are typically handled by treating them as a separate class or using imputation techniques.\n",
        "# Scikit-learn's BernoulliNB does not handle missing values directly, and missing values should be handled before using the classifier.\n",
        "\n",
        "# Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
        "# Yes, Gaussian Naive Bayes can be used for multi-class classification as it assumes that the continuous features follow a Gaussian (normal) distribution for each class.\n",
        "# It works well with multi-class problems where features are continuous and Gaussian-distributed.\n",
        "\n",
        "# Q5. Assignment: Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers on the \"Spambase\" dataset.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "url = \"https://www.openml.org/data/get_csv/345/Spambase.arff\"\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Prepare data\n",
        "X = data.drop('class', axis=1)  # Features\n",
        "y = data['class']  # Target variable\n",
        "\n",
        "# Step 3: Train-test split (use 70% for training, 30% for testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Create Naive Bayes classifiers\n",
        "bernoulli_nb = BernoulliNB()\n",
        "multinomial_nb = MultinomialNB()\n",
        "gaussian_nb = GaussianNB()\n",
        "\n",
        "# Step 5: Evaluate models using 10-fold cross-validation\n",
        "bernoulli_cv = cross_val_score(bernoulli_nb, X_train, y_train, cv=10, scoring='accuracy')\n",
        "multinomial_cv = cross_val_score(multinomial_nb, X_train, y_train, cv=10, scoring='accuracy')\n",
        "gaussian_cv = cross_val_score(gaussian_nb, X_train, y_train, cv=10, scoring='accuracy')\n",
        "\n",
        "# Step 6: Evaluate models on the test set\n",
        "bernoulli_nb.fit(X_train, y_train)\n",
        "multinomial_nb.fit(X_train, y_train)\n",
        "gaussian_nb.fit(X_train, y_train)\n",
        "\n",
        "bernoulli_y_pred = bernoulli_nb.predict(X_test)\n",
        "multinomial_y_pred = multinomial_nb.predict(X_test)\n",
        "gaussian_y_pred = gaussian_nb.predict(X_test)\n",
        "\n",
        "# Calculate performance metrics for each model\n",
        "def get_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"Precision\": precision_score(y_true, y_pred),\n",
        "        \"Recall\": recall_score(y_true, y_pred),\n",
        "        \"F1 Score\": f1_score(y_true, y_pred)\n",
        "    }\n",
        "\n",
        "bernoulli_metrics = get_metrics(y_test, bernoulli_y_pred)\n",
        "multinomial_metrics = get_metrics(y_test, multinomial_y_pred)\n",
        "gaussian_metrics = get_metrics(y_test, gaussian_y_pred)\n",
        "\n",
        "# Display the results\n",
        "print(\"Bernoulli Naive Bayes Metrics:\", bernoulli_metrics)\n",
        "print(\"Multinomial Naive Bayes Metrics:\", multinomial_metrics)\n",
        "print(\"Gaussian Naive Bayes Metrics:\", gaussian_metrics)\n",
        "\n",
        "# Step 7: Discussion and Conclusion\n",
        "# - Bernoulli Naive Bayes performs well for binary features, and is commonly used in text classification problems where features represent word presence.\n",
        "# - Multinomial Naive Bayes performs well with count-based features, such as word counts, and is often used for text classification.\n",
        "# - Gaussian Naive Bayes is suitable for continuous features, assuming they follow a normal distribution, and works well for real-valued features.\n",
        "\n",
        "# Discussion:\n",
        "# In this case, if the features are binary (presence or absence of specific words), Bernoulli Naive Bayes will likely perform the best.\n",
        "# Multinomial Naive Bayes might perform better if the features represent counts of occurrences.\n",
        "# Gaussian Naive Bayes could struggle because it assumes that the data is normally distributed, which might not be the case for this dataset.\n",
        "\n",
        "# Future Work Suggestions:\n",
        "# - Try feature engineering or feature selection to improve model performance.\n",
        "# - Experiment with other models, like Logistic Regression or Support Vector Machines.\n",
        "# - Hyperparameter tuning can be performed using GridSearchCV or RandomizedSearchCV to optimize the models further.\n"
      ]
    }
  ]
}